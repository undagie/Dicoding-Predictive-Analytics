# -*- coding: utf-8 -*-
"""Dicoding Predictive Analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x2ySUeq7ClKMuDoWDlpIrUvTmHsvTtT9

# Import library dan memuat dataset
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

# Muat dataset
# https://archive.ics.uci.edu/dataset/2/adult
df = pd.read_csv('adult.csv')

df.head(5)

df.tail(5)

df.count()

"""# Data Understanding

## Explanatory Data Analysis dan Visualisasi
"""

# Untuk fitur bertipe numeric
numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()
print(df[numerical_features].describe())

# Visualisasi fitur bertipe numeric
for feature in numerical_features:
    fig, ax = plt.subplots(1, 2, figsize=(14, 4))
    sns.histplot(df[feature], bins=30, ax=ax[0], kde=True)
    sns.boxplot(x=df[feature], ax=ax[1])
    plt.show()

# Menghitung korelasi antar fitur numerik
corr_matrix = df[numerical_features].corr()

# Visualisasi plotting headmap korelasi antar fitur numerik
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Heatmap of Numerical Features')
plt.show()

# Untuk fitur bertipe kategori
categorical_features = df.select_dtypes(include=['object']).columns.tolist()

for feature in categorical_features:
    print(df[feature].value_counts())

# Visualisasi fitur bertipe kategori
for feature in categorical_features:
    plt.figure(figsize=(10, 5))
    sns.countplot(y=feature, data=df)
    plt.show()

"""# Data Preparation

## Penanganan Missing Values
"""

# Mengganti nilai '?' dengan NaN untuk memudahkan penanganan missing values
df.replace('?', np.nan, inplace=True)

# Menghitung jumlah NaN di setiap kolom untuk mengindetifikasi missing values
missing_values = df.isnull().sum()

# Visualisasi missing values
plt.figure(figsize=(10, 6))
missing_values[missing_values > 0].plot(kind='bar')
plt.title('Missing Values in Each Column')
plt.xlabel('Columns')
plt.ylabel('Number of Missing Values')
plt.show()

missing_values[missing_values > 0]

# Mengisi missing values fitur 'workclass' and 'occupation' dengan 'Unknown'
df['workclass'].fillna('Unknown', inplace=True)
df['occupation'].fillna('Unknown', inplace=True)

# Mengisi missing values fitur 'native.country' dengan modusnya (nilai paling sering muncul)
native_country_mode = df['native.country'].mode()[0]
df['native.country'].fillna(native_country_mode, inplace=True)

# Memeriksa kembali missing values
print(df.isnull().sum())

"""## Penyederhanaan Distribusi Fitur"""

# Menentukan fitur numerik dan kategorikal untuk pemodelan
numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()
categorical_features = df.select_dtypes(include=['object']).columns.tolist()

# Menghapus fitur 'fnlwgt' dan 'income'
numeric_features.remove('fnlwgt')
categorical_features.remove('income')

# Menentukan minimal sample untuk negara yang akan diubah menjadi 'Other'
min_samples = 50

# Mengubah nama negara dengan nilai di bawah minimal menjadi 'Other'
df['native.country'] = df['native.country'].apply(lambda x: x if df['native.country'].value_counts()[x] > min_samples else 'Other')

# Menampilkan hasil untuk verifikasi distribusi setelah diubah
aggregated_native_country_distribution = df['native.country'].value_counts()
print(aggregated_native_country_distribution)

"""## Persiapan dan Splitting Dataset"""

# Membuat transformer untuk pra-pemrosesan
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ])

# Memisahkan fitur dan target dan membagi set pelatihan dan pengujian
X = df.drop(['income', 'fnlwgt'], axis=1)
y = df['income'].map({'<=50K': 0, '>50K': 1})
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Menerapkan pra-pemrosesan
X_train_preprocessed = preprocessor.fit_transform(X_train)
X_test_preprocessed = preprocessor.transform(X_test)

# Dictionary untuk menyimpan hasil akurasi
model_accuracies = {}

"""# Modeling

## Logistic Regression
"""

# Inisialisasi dan evaluasi Logistic Regression
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train_preprocessed, y_train)
y_pred_lr = lr_model.predict(X_test_preprocessed)
accuracy_lr = accuracy_score(y_test, y_pred_lr)
model_accuracies['Logistic Regression'] = accuracy_lr
print(f"Logistic Regression Accuracy: {accuracy_lr:.4f}")
print(classification_report(y_test, y_pred_lr))

"""## Decision Tree"""

# Inisialisasi dan evaluasi Decision Tree
dt_model = DecisionTreeClassifier()
dt_model.fit(X_train_preprocessed, y_train)
y_pred_dt = dt_model.predict(X_test_preprocessed)
accuracy_dt = accuracy_score(y_test, y_pred_dt)
model_accuracies['Decision Tree'] = accuracy_dt
print(f"Decision Tree Accuracy: {accuracy_dt:.4f}")
print(classification_report(y_test, y_pred_dt))

"""## Random Forest"""

# Inisialisasi dan evaluasi Random Forest
rf_model = RandomForestClassifier()
rf_model.fit(X_train_preprocessed, y_train)
y_pred_rf = rf_model.predict(X_test_preprocessed)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
model_accuracies['Random Forest'] = accuracy_rf
print(f"Random Forest Accuracy: {accuracy_rf:.4f}")
print(classification_report(y_test, y_pred_rf))

"""## XGBoost"""

# Inisialisasi dan evaluasi XGBoost
xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_model.fit(X_train_preprocessed, y_train)
y_pred_xgb = xgb_model.predict(X_test_preprocessed)
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
model_accuracies['XGBoost'] = accuracy_xgb
print(f"XGBoost Accuracy: {accuracy_xgb:.4f}")
print(classification_report(y_test, y_pred_xgb))

"""## SVM"""

# Inisialisasi dan evaluasi SVM
svm_model = SVC(probability=True)
svm_model.fit(X_train_preprocessed, y_train)
y_pred_svm = svm_model.predict(X_test_preprocessed)
accuracy_svm = accuracy_score(y_test, y_pred_svm)
model_accuracies['SVM'] = accuracy_svm
print(f"SVM Accuracy: {accuracy_svm:.4f}")
print(classification_report(y_test, y_pred_svm))

"""## Hasil Modeling"""

# Cetak hasil akurasi setiap model
for model, accuracy in model_accuracies.items():
    print(f"{model} Accuracy: {accuracy:.4f}")

# Membandingkan model berdasarkan akurasi
best_model = max(model_accuracies, key=model_accuracies.get)
print(f"\nModel dengan akurasi terbaik: {best_model} (Akurasi: {model_accuracies[best_model]:.4f})")

# Visualisasi akurasi semua model
plt.figure(figsize=(10, 6))
plt.barh(list(model_accuracies.keys()), list(model_accuracies.values()), color='skyblue')
plt.xlabel('Accuracy')
plt.title('Model Accuracy Comparison')
plt.show()

"""# Optimasi Dengan Hyperparameter Grid Search

## XGBoost
"""

from sklearn.model_selection import GridSearchCV

# Dictionary untuk menyimpan hasil akurasi terbaik dengan hyperparameter
best_model_accuracies = {}

# Definisikan kisi hyperparameter untuk XGBoost
xgb_param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1]
}

# Grid search untuk XGBoost
xgb_grid_search = GridSearchCV(xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
                               xgb_param_grid, cv=5, scoring='accuracy', n_jobs=-1)
xgb_grid_search.fit(X_train_preprocessed, y_train)
best_model_accuracies['XGBoost'] = xgb_grid_search.best_score_
print("Best hyperparameters for XGBoost:", xgb_grid_search.best_params_)
print("Best accuracy for XGBoost:", xgb_grid_search.best_score_)

"""## Random Forest"""

# Definisikan kisi hyperparameter untuk Random Forest
rf_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Grid search untuk Random Forest
rf_grid_search = GridSearchCV(RandomForestClassifier(), rf_param_grid, cv=5, scoring='accuracy', n_jobs=-1)
rf_grid_search.fit(X_train_preprocessed, y_train)
best_model_accuracies['Random Forest'] = rf_grid_search.best_score_
print("Best hyperparameters for Random Forest:", rf_grid_search.best_params_)
print("Best accuracy for Random Forest:", rf_grid_search.best_score_)

"""## SVM"""

# Definisikan kisi hyperparameter untuk SVM
svm_param_grid = {
    'C': [0.1, 1, 10],
    'gamma': ['scale', 'auto'],
    'kernel': ['rbf', 'linear']
}

# Grid search untuk SVM
svm_grid_search = GridSearchCV(SVC(probability=True), svm_param_grid, cv=5, scoring='accuracy', n_jobs=-1)
svm_grid_search.fit(X_train_preprocessed, y_train)
best_model_accuracies['SVM'] = svm_grid_search.best_score_
print("Best hyperparameters for SVM:", svm_grid_search.best_params_)
print("Best accuracy for SVM:", svm_grid_search.best_score_)

"""## Hasil Optimasi"""

# Membandingkan model berdasarkan akurasi terbaik dengan hyperparameter
best_hyper_model = max(best_model_accuracies, key=best_model_accuracies.get)
print(f"\nModel dengan akurasi terbaik setelah tuning hyperparameter: {best_hyper_model} (Akurasi: {best_model_accuracies[best_hyper_model]:.4f})")